{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Gymnasium"
      ],
      "metadata": {
        "id": "jzWfM2lY2Da5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yK9xqnu62DG2"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium\n",
        "!pip install \"gymnasium[atari, accept-rom-license]\"\n",
        "!apt-get install -y swig\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing the libraries"
      ],
      "metadata": {
        "id": "TXOGeikR2JPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "id": "x-vqoyGm2GBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the architecture of the Neural Network"
      ],
      "metadata": {
        "id": "51rLPw-B2LBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "  def __init__(self,action_size,seed=42):\n",
        "    super(Network,self).__init__()\n",
        "\n",
        "    self.seed=torch.manual_seed(seed)\n",
        "    self.conv1=nn.Conv2d(in_channels=3,out_channels=32,kernel_size=8,stride=4)\n",
        "    self.bn1=nn.BatchNorm2d(32)\n",
        "    self.conv2=nn.Conv2d(in_channels=32,out_channels=64,kernel_size=4,stride=2)\n",
        "    self.bn2=nn.BatchNorm2d(64)\n",
        "    self.conv3=nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,stride=1)\n",
        "    self.bn3=nn.BatchNorm2d(64)\n",
        "    self.conv4=nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,stride=1)\n",
        "    self.bn4=nn.BatchNorm2d(128)\n",
        "\n",
        "    self.fc1=nn.Linear(in_features=10*10*128,out_features=512)\n",
        "    self.fc2=nn.Linear(in_features=512,out_features=256)\n",
        "    self.fc3=nn.Linear(in_features=256,out_features=action_size)\n",
        "\n",
        "  def forward(self,state):\n",
        "   x = F.relu(self.bn1(self.conv1(state)))\n",
        "   x = F.relu(self.bn2(self.conv2(x)))\n",
        "   x = F.relu(self.bn3(self.conv3(x)))\n",
        "   x = F.relu(self.bn4(self.conv4(x)))\n",
        "   #reshapes the trensor so that the first dimension corresponding to the batch remains the same and other dimension are flattened\n",
        "   x=x.view(x.size(0),-1)\n",
        "\n",
        "   x=F.relu(self.fc1(x))\n",
        "   x=F.relu(self.fc2(x))\n",
        "   return self.fc3(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "GHJR1bE22NQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up the environment"
      ],
      "metadata": {
        "id": "7Sw4TWlW2O17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "#ensure the agent to use a simplified actions for pacman and make it easier to train\n",
        "env=gym.make('MsPacmanDeterministic-v0',full_action_space=False)\n",
        "state_shape=env.observation_space.shape\n",
        "state_size=env.observation_space.shape[0]\n",
        "number_actions=env.action_space.n\n",
        "\n",
        "print(\"state shape:\",state_shape)\n",
        "print(\"state size:\",state_size)\n",
        "print(\"number of actions:\",number_actions)"
      ],
      "metadata": {
        "id": "s7QCa_852QY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing the hyperparameters"
      ],
      "metadata": {
        "id": "7QN4lHCS2Rsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 5e-4\n",
        "minibatch_size = 64\n",
        "gamma = 0.99"
      ],
      "metadata": {
        "id": "ud9pNY7o2Te2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing the frames"
      ],
      "metadata": {
        "id": "RBRIbECT2U-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "#the frame is a numpy array and we want it to be an image\n",
        "def preprocess_frame(frame):\n",
        "  frame = Image.fromarray(frame)\n",
        "  preprocess = transforms.Compose([transforms.Resize((128,128)),transforms.ToTensor()])\n",
        "  return preprocess(frame).unsqueeze(0)"
      ],
      "metadata": {
        "id": "KfXBIjuW2VVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing the DCQN class"
      ],
      "metadata": {
        "id": "MNrb3Lbx2ZfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "  def __init__(self,action_size):\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.action_size = action_size\n",
        "\n",
        "    self.local_qnetwork = Network(action_size).to(self.device)\n",
        "    self.target_qnetwork = Network(action_size).to(self.device)\n",
        "\n",
        "    self.optimizer = optim.Adam(self.local_qnetwork.parameters(),lr=learning_rate)\n",
        "    self.memory = deque(maxlen = 10000)\n",
        "\n",
        "  def step(self, state,action,reward,next_state,done):\n",
        "    state=preprocess_frame(state)\n",
        "    next_state=preprocess_frame(next_state)\n",
        "    self.memory.append((state,action,reward,next_state,done))\n",
        "    #check the memory size is larger than the minibatch size because we learn on batches\n",
        "    if len(self.memory) > minibatch_size:\n",
        "      #we need to sample from the memory\n",
        "      experiences = random.sample(self.memory,k = minibatch_size)\n",
        "      self.learn(experiences,gamma)\n",
        "\n",
        "  def act(self,state,epsilon = 0.0):\n",
        "    #adding a new dimension for the batch at the begging of the torch sensor\n",
        "    state = preprocess_frame(state).to(self.device)\n",
        "    self.local_qnetwork.eval()\n",
        "    with torch.no_grad():\n",
        "      action_values = self.local_qnetwork(state)\n",
        "\n",
        "    self.local_qnetwork.train()\n",
        "\n",
        "    #epsilon greedy policy\n",
        "    if random.random() > epsilon:\n",
        "      return np.argmax(action_values.cpu().data.numpy())\n",
        "    else:\n",
        "      return random.choice(np.arange(self.action_size))\n",
        "\n",
        "  def learn(self,experiences,gamma):\n",
        "    states,actions,rewards,next_states,dones = zip(*experiences)\n",
        "    states = torch.from_numpy(np.vstack(states)).float().to(self.device)\n",
        "    actions = torch.from_numpy(np.vstack(actions)).long().to(self.device)\n",
        "    rewards = torch.from_numpy(np.vstack(rewards)).float().to(self.device)\n",
        "    next_states = torch.from_numpy(np.vstack(next_states)).float().to(self.device)\n",
        "    dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float().to(self.device)\n",
        "\n",
        "    #we need to add a dimension for the batch but this time at dimension 1 for the action values\n",
        "    next_q_targets = self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "    q_targets = rewards + (gamma * next_q_targets * (1-dones))\n",
        "    q_expected = self.local_qnetwork(states).gather(1,actions)\n",
        "    loss = F.mse_loss(q_expected,q_targets)\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Xh64h4ly2c08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent=Agent(number_actions)"
      ],
      "metadata": {
        "id": "l5Sc5yvW2fWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the DCQN agent"
      ],
      "metadata": {
        "id": "abhBaxcY2Zci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_episodes = 2000\n",
        "maximum_number_timesteps_per_episode = 10000\n",
        "epsilon_start_value = 1.0\n",
        "epsilon_final_value = 0.01\n",
        "epsilon_decay_rate = 0.995\n",
        "epsilon = epsilon_start_value\n",
        "scores_on_100_episodes = deque(maxlen=100)"
      ],
      "metadata": {
        "id": "Teu1HHCC2n3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(1,number_episodes + 1):\n",
        "  #returns the inital state\n",
        "  state,_ = env.reset()\n",
        "  #cumulative reward\n",
        "  score=0\n",
        "  for t in range(maximum_number_timesteps_per_episode):\n",
        "    action = agent.act(state,epsilon)\n",
        "    next_state,reward,done,_ ,_= env.step(action)\n",
        "    agent.step(state,action,reward,next_state,done)\n",
        "    state = next_state\n",
        "    score += reward\n",
        "    if done:\n",
        "      break\n",
        "  scores_on_100_episodes.append(score)\n",
        "  epsilon = max(epsilon_final_value,epsilon * epsilon_decay_rate)\n",
        "  print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)), end=\"\")\n",
        "  if episode % 100 == 0:\n",
        "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)))\n",
        "  if np.mean(scores_on_100_episodes) >= 500.0:\n",
        "    print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(episode-100, np.mean(scores_on_100_episodes)))\n",
        "    torch.save(agent.local_qnetwork.state_dict(), 'checkpoint.pth')\n",
        "    break"
      ],
      "metadata": {
        "id": "EhXNJSyv2oMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing the results"
      ],
      "metadata": {
        "id": "D4GM0DgE2ZGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import imageio\n",
        "from IPython.display import HTML, display\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "\n",
        "def show_video_of_model(agent, env_name):\n",
        "    env = gym.make(env_name, render_mode='rgb_array')\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    frames = []\n",
        "    while not done:\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "        action = agent.act(state)\n",
        "        state, reward, done, _, _ = env.step(action)\n",
        "    env.close()\n",
        "    imageio.mimsave('video.mp4', frames, fps=30)\n",
        "\n",
        "show_video_of_model(agent, 'MsPacmanDeterministic-v0')\n",
        "\n",
        "def show_video():\n",
        "    mp4list = glob.glob('*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "show_video()"
      ],
      "metadata": {
        "id": "Koybz5JJ2wWq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}